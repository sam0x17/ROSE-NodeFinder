---------------------------------------------------------------------------
			     Introduction

This directory contains commands for finding clusters of binary
functions that are semantically similar.  The commands are prefixed
with a two-digit number to help you determine which order they should
be run, but the database can be built incrementally, so the order is
not strict.  Generally speaking though, commands with higher numbers
should be run after the lower-numbered commands have been run at least
once.  All commands support a "--help" (and "-h") switch. The grouping
of commands is:

    00-09: Initializing database (non-incremental)
    10-19: Initializing database (incremental)
    20-29: Running tests
    30-39: Analyzing test results
    40-79: Reserved
    80-89: Database maintenance
    90-99: Querying results

All information is stored in a database. We found that SQLite3 was
insufficient for our needs, mainly because it had extremely poor
performance in parallel and some of our more complex queries too a
long time to run.  Therefore, we recommend using PostgreSQL for all
but toy examples.  Each command takes a database name or URL, and the
format of that URL determines which driver is used.

When using PostgreSQL, when a command fails it will usually leave the
database unchanged.  The only exceptions are that a few of the
long-running commands will perform a checkpoint at regular intervals
so that if they fail all is not lost.


---------------------------------------------------------------------------
			    Prerequisites

ROSE needs to be configured with support for either SQLite3 or
PostgreSQL. SQLite3 support is configured via "--with-sqlite3" switch
on the "configure" script; PostgreSQL support is enabled automatically
if the libpqxx library is installed on the system.  The libpqxx
library can be installed on Debian-based systems (like Ubuntu) with:

    $ sudo apt-get install libpqxx3-dev

---------------------------------------------------------------------------
                     Configuring the PostgreSQL server

Note: the server does not need to be running on the same machine as the
analysis, so this step is optional.  Install the server with:

    $ sudo apt-get install postgresql

If necessary, shutdown the server, edit the /etc/postgresql config files,
move data files to a file system with more space, and restart the server.
Debian puts the server's data directory under /var, which is often too small.

Create a database role which is the same as your user name:

    $ su
    # su postgres
    $ createuser

Test that things work by creating and then dropping a database as the
normal user (not postgres or root):

    $ createdb checking
    $ dropdb checking

---------------------------------------------------------------------------
			     The Database

The schema is described in Schema.sql, which is compiled into the
00-create-schema tool, which when run drops and recreates all the
tables described in that file.

Most database access is performed from C++ using ROSE's SqlDatabase
name space. This API supports both SQLite3 and PostgreSQL, so all SQL
must be written to be portable across at least those two drivers.

The driver is chosen automatically based on the URL supplied as an
argument to a command.  The first few characters of a database URL are
either "postgresql://" or "sqlite3://".  In the SQLite3 case, this is
followed by the name of the file that contains (or will be created to
contain) the database. The PostgreSQL is more complex:

    * An optional user name followed by an '@' character.  The '@' may
      be preceded by a plain-text password that is separated from the
      user name by a ':'.  The password itself must not contain ':' or
      '@' characters.  If the user name (and thus password) are not
      present then the final '@' should also not be present.

    * An optional host-name or IP address.

    * An optional TCP port number or name preceded by a ':'.

    * An optional database name preceded by a '/'.  This slash is in
      addition to the two at the beginning of the URL, so that if no
      username, host or IP address, or port number are present the URL
      will contain three consecutive slashes, as in "postgresql:///x"
      to connect to a database named "x".

The URL ends with zero or more parameters, the first of which is
introduced by a '?' and the subsequent by '&'.  Each parameter is a
name followed by an optional '=' and value.  Both drivers understand a
\"debug\" parameter, which causes SqlDatabase to emit tracing
information on the standard error stream.

If the connection string isn't a recognized URL but looks like a file
name, then SQLite3 is used.

SQLite3 database files are created the first time they're used, but
PostgreSQL files need to be created on the PostgreSQL server before
they can be used.  See the manpage for "createuser" and
"createdb". Due to the potentially large amount of data that needs to
be copied between the analysis tools and the database, especially for
the 25-run-tests tool, performance is best when the PostgreSQL server
runs on localhost.


---------------------------------------------------------------------------
			Preparing the Database

The examples here use a PostgreSQL database named "example" running on
the localhost and using the user's login credentials for the database
role and password.

The first step (after the PostgreSQL user and database are created) is
to initialize the schema.  By "schema" we mean the set of tables as a
whole, not a true PostgreSQL schema since that concept doesn't exist
for SQLite3.  The command to do this is:

    $ 00-create-schema postgresql:///example

The next two steps (in any order) are to populate those tables with
input groups and functions.  Each input group is a collection of
integer and pointer values that are supplied as inputs to a test. The
input groups can be created incrementally and there are quite a few
switches for this tool.  An example command is:

    $ 10-generate-inputs --ngroups=100 postgresql:///example

The functions can also be added incrementally at any time, one binary
specimen at a time.  This tool adds all the functions it finds in the
specimen (and optionally in dynamically linked libraries) regardless
of whether the function will eventually be tested.  It also adds
various other binary information to the database such as the AST, call
graph, the specimen itself, all dynamically-linked libraries, assembly
listings, source code files, etc.  Most of this is done automatically,
so there are only a couple command-line switches.  Here's an example
command that adds the specimen named "a.out":

    $ 11-add-functions postgresql:///example a.out


---------------------------------------------------------------------------
			    Running Tests

Once the database has some input groups and functions you can choose
which tests to run.  A "test" is a pair consisting of one function and
one input groups and which produces a single output group. Each output
group will contain various information about the test results,
including the values produced as outputs, the number of instructions
executed, certain kinds of function calls, system calls, etc.  Most of
this output information is turned on/off by conditional compilation in
25-run-tests.  The 20-get-pending-tests command produces a work-list
on standard output which is eventually fed into the 25-run-tests
command. There are quite a few siwtches to control which tests to
select, but here's an example that selects all functions that have at
least 100 instructions:

    $ 20-get-pending-tests --size=100 postgresql:///example >worklist

The 25-run-tests reads a worklist from standard input and runs each
test, one at a time.  Since this is by far the most expensive step,
this tool is designed to work in parallel.  Typically one obtains a
worklist with 20-get-pending-tests, then splits it into parts, then
executes a 25-run-tests for each part.  The 25-run-tests tool has a
number of switches to control how things run.  Here's an example:

    $ 25-run-tests postgresql:///example <worklist


---------------------------------------------------------------------------
			Calculating Similarity

The next step is to compare the outputs from the tests in order to
measure how similar functions are to each other, which is done by the
32-func-similarity.  This command has a number of switches that
control the algorithms used to measure similarity.  By default, it
only computes similarity for those pairs of functions for which
similarity is not yet known. Function similarity is a reflexive and
symmetric relationship whose range is zero through one (one indicating
equality), so the tool only considers pairs of functions where the lhs
function ID is less than the rhs function ID. Here's a simple example:

    $ 32-func-similarity postgresql:///example

The 35-clusters-from-pairs tool looks at the pair-wise similarity
between functions and tries to organize them into clusters where all
functions in a cluster have a user-specified minimum similarity with
all other functions in that cluster.  Singleton clusters are not
produced since that would be redundant with the fact that similarity
is reflexive.  If the threshold of similarity is one (equality) then
the similarity relationship is transitive and each function will
appear in exactly zero (a singleton function) or one cluster;
otherwise the relationship is non-transitive and a function could
appear in more than one cluster.

    $ 35-clusters-from-pairs --threshold=0.75 \
        postgresql:///example semantic_funcsim semantic_clusters
      


---------------------------------------------------------------------------
			   Querying Results

Finally, the commands in group 90-99 are used to query the
results. Most common, complex queries are implemented as tools in this
command group, but users are obviously free to talk directly to the
database server as well.  Here are some examples:

    $ 90-list-clusters postgresql:///example

    $ 90-list-function postgresql:///example main
    $ 90-list-function postgresql:///example a.out
    $ 90-list-function postgresql:///example 0x08050060
    $ 90-list-function postgresql:///example 123

The source tree also has a few SQL queries stored in various *.sql
files. These are typically run like this:

    $ psql -f failure-rate.sql example 2>&1 |less


---------------------------------------------------------------------------
			Incremental Operation

The commands do not need to run in the order presented.  For instance,
one could initialize the database and populate it with inputs. Each
time a new specimen arrives the user can add its functions to the
database.  Another machine could be dedicated to running tests in
parallel by querying the database (20-get-pending-tests) at regular
intervals, processing the resulting work lists (25-run-tests), and
updating the similarity relationship (32-func-similarity).  End users
can create clusters (35-clusters-from-pairs) whenever they need them,
and run queries.  In fact, if PostgreSQL is used, the users and
workers don't even need to be on the same machine.

---------------------------------------------------------------------------
			   Reproducibility

All tools that modify the database record their command-line and date
in the semantic_history table. Many of the tables also have a "cmd"
column to indicate which command created or modified that row of the
table. (Unfortunately, we can't record user-issued SQL that that
modifies the database.)

Although input group values are created randomly, the analysis uses
its own linear congruential generators.  By default, the values of an
input group are generated based on the input group's ID number, so it
doesn't matter how many commands are used to populate that part of the
database.

Most database-modifying commands run in their own transaction so that
if they fail the database is not modified (at least when using
PostgreSQL). The exceptional commands are long-running commands that
periodically checkpoint, such as 25-run-tests. These commands will
have reproducible results if they run to completion or are restarted
when they fail.

Time-limits are specified in terms of number of instructions rather
than actual time in order to avoid nondeterministic results for tests
that time out.


---------------------------------------------------------------------------
				Safety

The analysis doesn't actually truly run each test, but rather
interprets the test instructions using ROSE's concrete domain for
instruction semantics. Since all system calls are intercepted and
not executed, the specimen can do no damage.  The worst that can
happen is that the specimen causes the analysis to use an inordinate
amount of memory or CPU time, but the analysis sets limits on these
and terminates the test if it exceeds the limits.

Also, the analysis is able to process any ROSE-supported binary format
on any ROSE-supported host.  This means, for example, that Windows PE
specimens can be analyzed on a Linux host, further protecting the host
from a malicious specimen.


---------------------------------------------------------------------------
			What makes this hard?

In a few words: time, size, details, and debugging.  And the fact that
optimizing compilers tend to be good adversaries.  Here are some of
the things we needed to overcome:

1.  (Size) Each specimen typically contains hundres of instructions
    (thousands when dynamically linked).  Each function is fuzz tested
    hundreds of times.  Each test needs reproducible sequences of
    inputs, and each sequence can contain thousands of values. Each
    test produces an output group, each of which contains potentially
    thousands of values. (One of our runs tested each function more
    than 40,000 times.)

2.  (Size) The similarity of two functions is based on the similarity
    of the output groups they created for each time they were tested
    on a common input group.  This computation is too complex to be
    carried out on the database server, thus output data needs to be
    downloaded to processors written in C/C++.

3.  (Size) We currently compute similarity for all pairs of functions
    and similarity for any single pair is calculated over all their
    outputs.  (One of our tests computed similarity for 1.6x10^9 pairs
    of output groups.)

3b. (Size) Presenting the output as a linear text file became
    unreasonable due to the size and the types of questions we wanted
    to answer.  Each query required either writing custom C/C++ code
    to accumulate and present the results from a subsequent run, or
    Perl code to parse the output of a previous run.  Therefore we
    began using an SQLite3 database.

4.  (Time) Functions are tested by interpreting each machine
    instruction. The first version of clone detection used ROSE's x86
    simulator patched to perform mixed interpretation.  This turned
    out to be overkill and slow.  We moved everything to use ROSE's
    API1 instruction semantics (the template-based stuff) and were
    able to obtain close to 1 million instructions/second.

5.  (Time) Running at 1 million instructions/second still didn't fully
    utilize a modern mutli-core desktop machine, so we parallelized
    the testing.  Robb's office machine runs at an aggregate rate of
    around 10 million instructions/second.

6.  (Time) Our original design used SQLite3 as the underlying
    database.  This turned out to be a problem because of missing
    functionality in SQLite3 and very poor performance in a parallel
    work load.  We added support for PostgreSQL for performance
    reasons, but kept the SQLite3 support for users that don't want to
    deal with setting up a PostgreSQL server.

7.  (Size) Testing produces a large number of table records for the
    output values and the optional tracing information. Special
    considerations needed to be made to get these uploaded to the
    database server in an efficient manner.

8.  (Detail) We needed a way to track (within the database) what
    commands were used to create the data.

9.  (Detail) We need to be able to incrementally build the database as
    new specimens become available.  We also needed a way to re-run
    individual tests for debugging purposes without requiring that all
    tests be rerun.

10. (Time) A specimen should not have to be re-disassembled every time
    we need to access it.  Therefore, we disassemble once and then
    store the binary AST in the database.  We also store instructions,
    the binary itself, the call graph, etc.

11. (Debug) Since most users program in a high-level language and are
    not accustomed to looking at x86 assembly language, we also need
    to store the source code and build custom ways of producing
    listings that contain both source and assembly.

12. (Debug) Resulting datasets quickly became too large to upload over
    a slow network. They can now be posted on Internet-facing
    PostgreSQL servers.  In fact, the analysis can update the server
    directly if desired.  Access can be controlled through the usual
    PostgreSQL mechanisms (authentication and roles).


   

---------------------------------------------------------------------------
		    History, Acknowledgments, etc.

This analysis was born as part of the x86 simulator project and later
extracted from the simulator and moved to its own home.  It was
changed to use ROSE's concrete instruction semantics,
BinaryAnalysis::InstructionSemantics::PartialSymbolicSemantics, which
resulted in a very large speed improvement at the expense of not being
able to allow the specimen to perform Linux i386 system calls (but
that was probably a bad idea anyway when fuzz testing).

The idea to store results in an SQLite3 database came from the
syntactic clone analysis written by a student.  At that time, ROSE
only supported SQLite3.

We found that SQLite3 was deficient in terms of both speed and
functionality and grew tired of waiting for results and writing
convoluted SQL to work around missing features.  In June 2013 ROSE
gained the SqlDatabase API which allowed portable, C++-oriented access
to SQLite3 and PostgreSQL, and all of the analysis was switched to use
this API. At about the same time, the monolithic analysis was split
into many smaller tools. Parallel testing speed increased by more than
two orders of magnitude and many of our other queries ran faster also.

Our first large successful run was a comparison of grep compiled with
GCC using no optimization (-O0) and full optimization (-O3), using
40,320 permuations of an input group having eight integer values. This
run took about 14 hours on 10 CPUs, ran 4.7 million tests on 116
functions, and processed just over half a trillion x86 instructions at
approx 10 million instructions per second. The database dump (SQL from
pg_dump) was 1.2 GB.
